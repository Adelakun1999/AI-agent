{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23584a36-19f0-4a73-9bea-d99373be0006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import frontmatter\n",
    "import io\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97b8a715-ee7e-4809-85dc-1aeecdcc8f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://codeload.github.com/DataTalksClub/faq/zip/refs/heads/main'\n",
    "resp = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9687164-377b-4314-b8c8-d661586560cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_data = []\n",
    "\n",
    "# Create a ZipFile object from the downloaded content\n",
    "zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "for file_info in zf.infolist():\n",
    "    filename = file_info.filename.lower()\n",
    "    # Only process markdown files\n",
    "    if not filename.endswith('.md'):\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "    # Read and parse each file\n",
    "    with zf.open(file_info) as f_in:\n",
    "        content = f_in.read().decode('utf-8' , errors=\"ignore\")\n",
    "        post = frontmatter.loads(content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = filename\n",
    "        repository_data.append(data)\n",
    "\n",
    "zf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02f020b-7111-4dda-93f0-df7c39b6d73e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acde970c-7b0a-42a4-b97a-979b0f5ae243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '9e508f2212',\n",
       " 'question': 'Course: When does the course start?',\n",
       " 'sort_order': 1,\n",
       " 'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       " 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repository_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "310b083d-c2b0-4ace-9d30-dfb6d8e54f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56925457-34e1-4a47-8208-6d24e6a0d0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_cookbooks = read_repo_data('athina-ai' , 'rag-cookbooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56d0ef36-25e0-44d5-8e12-15c7fd80553f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ documents: 1219\n",
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f5315f-9022-4f23-9e6f-529191505bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc3175ce-6676-4b93-a875-f251f78b3d89",
   "metadata": {},
   "source": [
    "## Day 2 Chunking  and Intelligent Processing for Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51ac073-3879-4c7e-a59d-914b178c2714",
   "metadata": {},
   "source": [
    "## 1 Simple Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59a2793e-ddbf-434b-bba5-df748ea25b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq , size , step):\n",
    "    if size <=0 or step <=0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0 , n , step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start' : i , 'chunk' : chunk})\n",
    "        if i + size >=n:\n",
    "            break \n",
    "            \n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "557b2621-4fe0-4834-a2f1-8615de2580a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = sliding_window(rag_cookbooks[0]['content'] , 400 , 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "696bb39e-67fa-4e3b-9bfa-ffddcea822fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c521220b-005e-4497-af1f-c0da84870da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_chunks = []\n",
    "\n",
    "for doc in rag_cookbooks:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content ,500 , 200)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    RAG_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b9959a2-2cde-4d02-9cfa-96934af9373d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(RAG_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ef5e59-65d4-4cac-94ed-6b4d5cb92165",
   "metadata": {},
   "source": [
    "## 2 . Splitting by Paragraphs and Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b5e34010-9339-4c27-98c8-c90b0c4fead7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "    \n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    # This regex matches markdown headers\n",
    "    # For level 2, it matches lines starting with \"## \"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    # Split and keep the headers\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "        header = header.strip()\n",
    "\n",
    "        # Get the content after this header\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "34cc6c4e-2ef6-438c-8f8c-474ca0a5a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chunks = []\n",
    "\n",
    "for doc in rag_cookbooks:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        rag_chunks.append(section_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4c7d011-24de-4b95-855c-d3473adae93f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rag_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98eb3d7b-a6ff-4fa8-b215-af689f92fac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8f7a44f-ad05-41d1-9973-c83d091db4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e636283-e206-4939-816d-520398357d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI(api_key= api_key)\n",
    "\n",
    "\n",
    "def llm(prompt, model='gpt-4o-mini'):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model='gpt-4o-mini',\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d56dae7-0af4-4a0d-921f-560505537a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8510615fda014ee9a36f79a2b3b706ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "rag_chunks = []\n",
    "\n",
    "for doc in tqdm(rag_cookbooks):\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        rag_chunks.append(section_doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9799620-89ec-40d7-8388-8160a1ffb088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': 'rag-cookbooks-main/README.md',\n",
       " 'section': '## Advanced + Agentic RAG Cookbooks\\n\\nWelcome to the comprehensive collection of advanced + agentic Retrieval-Augmented Generation (RAG) techniques.'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34119bdc-765d-40eb-a601-7d728447a1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rag_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1710880-4914-43c5-a4ae-aa17d2f07b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': 'rag-cookbooks-main/README.md',\n",
       " 'section': '## RAG Evaluation📊\\n\\nEvaluating RAG applications is essential for understanding their effectiveness. This evaluation checks the accuracy and relevance of RAG systems, helping to optimize performance and build confidence for real-world applications.\\n\\n![evals diagram](https://github.com/user-attachments/assets/65c2b5af-a931-40c5-b006-87567aef019f)'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chunks[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b029acf-1a22-4441-8f57-03c91ec9bdae",
   "metadata": {},
   "source": [
    "### Text Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e56b3cb3-224a-434f-a2b9-a64ccde0bd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9de410ec-38fc-494c-a2fb-bbd0cbd9c2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x2a73c208650>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = Index(\n",
    "    text_fields=  [\"chunk\", \"title\", \"description\", \"filename\"],\n",
    "    keyword_fields= []\n",
    "    \n",
    ")\n",
    "\n",
    "index.fit(RAG_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c84f1859-94c0-417c-913f-9bb040617fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 3800,\n",
       "  'chunk': 'al with generative models by checking their accuracy and relevance. This evaluation helps improve RAG applications in tasks like text summarization, chatbots, and question-answering. It also identifies areas for improvement, ensuring that these systems provide trustworthy responses as information changes. Overall, effective evaluation helps optimize performance and builds confidence in RAG applications for real-world use. These notebooks contain an end-to-end RAG implementation + RAG evaluation ',\n",
       "  'filename': 'rag-cookbooks-main/README.md'},\n",
       " {'start': 4000,\n",
       "  'chunk': 's areas for improvement, ensuring that these systems provide trustworthy responses as information changes. Overall, effective evaluation helps optimize performance and builds confidence in RAG applications for real-world use. These notebooks contain an end-to-end RAG implementation + RAG evaluation part in Athina AI.\\n\\n![evals diagram](https://github.com/user-attachments/assets/65c2b5af-a931-40c5-b006-87567aef019f)\\n\\n\\n\\n## Advanced RAG Techniques⚙️\\nHere are the details of all the Advanced RAG techn',\n",
       "  'filename': 'rag-cookbooks-main/README.md'},\n",
       " {'start': 3400,\n",
       "  'chunk': 'mponents of RAG allow the model to access up-to-date, accurate information and generate responses based on external knowledge. However, to ensure RAG systems are functioning effectively, it’s essential to evaluate their performance.\\n\\n## RAG Evaluation📊\\nEvaluating RAG applications is important for understanding how well these systems work. We can see how effectively they combine information retrieval with generative models by checking their accuracy and relevance. This evaluation helps improve RA',\n",
       "  'filename': 'rag-cookbooks-main/README.md'},\n",
       " {'start': 3600,\n",
       "  'chunk': 'l to evaluate their performance.\\n\\n## RAG Evaluation📊\\nEvaluating RAG applications is important for understanding how well these systems work. We can see how effectively they combine information retrieval with generative models by checking their accuracy and relevance. This evaluation helps improve RAG applications in tasks like text summarization, chatbots, and question-answering. It also identifies areas for improvement, ensuring that these systems provide trustworthy responses as information ch',\n",
       "  'filename': 'rag-cookbooks-main/README.md'},\n",
       " {'start': 4200,\n",
       "  'chunk': 'tions for real-world use. These notebooks contain an end-to-end RAG implementation + RAG evaluation part in Athina AI.\\n\\n![evals diagram](https://github.com/user-attachments/assets/65c2b5af-a931-40c5-b006-87567aef019f)\\n\\n\\n\\n## Advanced RAG Techniques⚙️\\nHere are the details of all the Advanced RAG techniques covered in this repository.\\n\\n| Technique                    | Tools                        | Description                                                       | Notebooks |\\n|--------------------',\n",
       "  'filename': 'rag-cookbooks-main/README.md'},\n",
       " {'start': 3200,\n",
       "  'chunk': \"information needed to generate accurate responses.\\n\\n**Generate:** Finally, the combined query and prompt are passed to the model, which then generates the final response to the user's query.\\n\\nThese components of RAG allow the model to access up-to-date, accurate information and generate responses based on external knowledge. However, to ensure RAG systems are functioning effectively, it’s essential to evaluate their performance.\\n\\n## RAG Evaluation📊\\nEvaluating RAG applications is important for un\",\n",
       "  'filename': 'rag-cookbooks-main/README.md'},\n",
       " {'start': 1600,\n",
       "  'chunk': 'ng proper evaluation methods can be challenging. This repository simplifies the process by offering ready-to-use implementations and guidance on how to evaluate them.\\n>[!NOTE]\\n>This repository starts with naive RAG as a foundation and progresses to advanced and agentic techniques. It also includes research papers/references for each RAG technique, which you can explore for further reading.\\n\\n## Introduction to RAG💡\\nLarge Language Models are trained on a fixed dataset, which limits their ability t',\n",
       "  'filename': 'rag-cookbooks-main/README.md'},\n",
       " {'start': 1200,\n",
       "  'chunk': 'from reliable sources and transforming it into useful answers. This repository covers the most effective advanced + agentic RAG techniques with clear implementations and explanations.\\n\\nThe main goal of this repository is to provide a helpful resource for researchers and developers looking to use advanced RAG techniques in their projects. Building these techniques from scratch takes time, and finding proper evaluation methods can be challenging. This repository simplifies the process by offering ',\n",
       "  'filename': 'rag-cookbooks-main/README.md'},\n",
       " {'start': 1400,\n",
       "  'chunk': 'f this repository is to provide a helpful resource for researchers and developers looking to use advanced RAG techniques in their projects. Building these techniques from scratch takes time, and finding proper evaluation methods can be challenging. This repository simplifies the process by offering ready-to-use implementations and guidance on how to evaluate them.\\n>[!NOTE]\\n>This repository starts with naive RAG as a foundation and progresses to advanced and agentic techniques. It also includes r',\n",
       "  'filename': 'rag-cookbooks-main/README.md'},\n",
       " {'start': 4400,\n",
       "  'chunk': '006-87567aef019f)\\n\\n\\n\\n## Advanced RAG Techniques⚙️\\nHere are the details of all the Advanced RAG techniques covered in this repository.\\n\\n| Technique                    | Tools                        | Description                                                       | Notebooks |\\n|---------------------------------|------------------------------|--------------------------------------------------------------|-----------|\\n| Naive RAG      | LangChain, Pinecone, Athina AI                    | Combines',\n",
       "  'filename': 'rag-cookbooks-main/README.md'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"RAG evaluation\"\n",
    "results = index.search(query)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b12e77b-a5cc-43c8-bdfa-b249abec9935",
   "metadata": {},
   "source": [
    "### Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6bac7ca-6f47-49ab-979f-b68309e788c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "945dcc1b-8f4f-47cb-a66a-eb6e6229d0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e25b834-4536-406b-815d-590277cea397",
   "metadata": {},
   "outputs": [],
   "source": [
    "record = RAG_chunks[20]\n",
    "\n",
    "text = record['chunk']\n",
    "\n",
    "v_doc = embedding_model.encode(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0a11a2e-4561-4092-86d9-5218ad02156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Evaluating rag application\"\n",
    "\n",
    "v_query = embedding_model.encode(query)\n",
    "\n",
    "similarity = v_query.dot(v_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74b5bbfa-b9e7-4cdf-8f9c-b6f123232446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b6073ff8eb47a488c632d0f6cc1cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np \n",
    "\n",
    "faq_embeddings = []\n",
    "\n",
    "for d in tqdm(RAG_chunks):\n",
    "    text =d['chunk']\n",
    "    v = embedding_model.encode(text)\n",
    "    faq_embeddings.append(v)\n",
    "faq_embeddings = np.array(faq_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69da87a8-7e6e-41d7-8872-024554fcad97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15881318, -0.041525  ,  0.06855938, ...,  0.05195213,\n",
       "        -0.02086912,  0.00230424],\n",
       "       [ 0.15370825, -0.04568202,  0.05077348, ...,  0.02697159,\n",
       "        -0.0393999 ,  0.01631664],\n",
       "       [ 0.0958629 , -0.0182435 ,  0.02079821, ...,  0.05082281,\n",
       "        -0.00261695,  0.01596041],\n",
       "       ...,\n",
       "       [ 0.05444733,  0.0338323 ,  0.00745371, ...,  0.06647051,\n",
       "         0.03916425,  0.03215628],\n",
       "       [ 0.06413162,  0.00811296, -0.00165808, ...,  0.03467955,\n",
       "         0.04873566,  0.01943674],\n",
       "       [ 0.07936952,  0.04640346, -0.02640165, ...,  0.00537769,\n",
       "         0.02057582, -0.00446462]], shape=(53, 768), dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faq_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a6a5b51-61cb-4ffe-81c1-dcc11cc1c894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x2a748cc0bf0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import VectorSearch\n",
    "faq_vindex = VectorSearch()\n",
    "\n",
    "faq_vindex.fit(faq_embeddings , RAG_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b5256ce-02ad-4370-ad93-411c200f5870",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the main four components in RAG\"\n",
    "q = embedding_model.encode(query)\n",
    "result = faq_vindex.search(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37177356-cf01-4272-9b47-8304d8d07724",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 2400,\n",
       "  'chunk': \"ternal documents to improve the LLM's responses through in-context learning. RAG ensures that the information provided by the LLM is not only contextually relevant but also accurate and up-to-date.\\n\\n![final diagram](https://github.com/user-attachments/assets/508b3a87-ac46-4bf7-b849-145c5465a6c0)\\n\\nThere are four main components in RAG:\\n\\n**Indexing:** First, documents (in any format) are split into chunks, and embeddings for these chunks are created. These embeddings are then added to a vector sto\",\n",
       "  'filename': 'rag-cookbooks-main/README.md'},\n",
       " {'start': 2600,\n",
       "  'chunk': \"[final diagram](https://github.com/user-attachments/assets/508b3a87-ac46-4bf7-b849-145c5465a6c0)\\n\\nThere are four main components in RAG:\\n\\n**Indexing:** First, documents (in any format) are split into chunks, and embeddings for these chunks are created. These embeddings are then added to a vector store.\\n\\n**Retriever:** Then, the retriever finds the most relevant documents based on the user's query, using techniques like vector similarity from the vector store.\\n\\n**Augment:** After that, the Augmen\",\n",
       "  'filename': 'rag-cookbooks-main/README.md'},\n",
       " {'start': 4400,\n",
       "  'chunk': '006-87567aef019f)\\n\\n\\n\\n## Advanced RAG Techniques⚙️\\nHere are the details of all the Advanced RAG techniques covered in this repository.\\n\\n| Technique                    | Tools                        | Description                                                       | Notebooks |\\n|---------------------------------|------------------------------|--------------------------------------------------------------|-----------|\\n| Naive RAG      | LangChain, Pinecone, Athina AI                    | Combines',\n",
       "  'filename': 'rag-cookbooks-main/README.md'},\n",
       " {'start': 7400,\n",
       "  'chunk': 'ctured RAG     | LangChain, LangGraph, FAISS, Athina AI, Unstructured                    | This method designed to handle documents that combine text, tables, and images.| [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/athina-ai/rag-cookbooks/blob/main/advanced_rag_techniques/basic_unstructured_rag.ipynb) |\\n\\n## Agentic RAG Techniques⚙️\\nHere are the details of all the Agentic RAG techniques covered in this repository.\\n\\n| Techn',\n",
       "  'filename': 'rag-cookbooks-main/README.md'},\n",
       " {'start': 800,\n",
       "  'chunk': 't%20on%20GitHub:%20https://github.com/athina-ai/rag-cookbooks)\\n\\n>If you find this repository helpful, please consider giving it a star⭐️\\n\\n# Advanced + Agentic RAG Cookbooks👨🏻\\u200d💻\\nWelcome to the comprehensive collection of advanced + agentic Retrieval-Augmented Generation (RAG) techniques.\\n\\n## Introduction🚀\\nRAG is a popular method that improves accuracy and relevance by finding the right information from reliable sources and transforming it into useful answers. This repository covers the most effec',\n",
       "  'filename': 'rag-cookbooks-main/README.md'},\n",
       " {'start': 7800,\n",
       "  'chunk': 'niques⚙️\\nHere are the details of all the Agentic RAG techniques covered in this repository.\\n\\n| Technique                    | Tools                        | Description                                                       | Notebooks |\\n|---------------------------------|------------------------------|--------------------------------------------------------------|-----------|\\n| Basic Agentic RAG      | LangChain, FAISS, Athina AI                    | Agentic RAG uses AI agents to find and genera',\n",
       "  'filename': 'rag-cookbooks-main/README.md'},\n",
       " {'start': 7600,\n",
       "  'chunk': 'lab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/athina-ai/rag-cookbooks/blob/main/advanced_rag_techniques/basic_unstructured_rag.ipynb) |\\n\\n## Agentic RAG Techniques⚙️\\nHere are the details of all the Agentic RAG techniques covered in this repository.\\n\\n| Technique                    | Tools                        | Description                                                       | Notebooks |\\n|---------------------------------|----------------------------',\n",
       "  'filename': 'rag-cookbooks-main/README.md'},\n",
       " {'start': 4200,\n",
       "  'chunk': 'tions for real-world use. These notebooks contain an end-to-end RAG implementation + RAG evaluation part in Athina AI.\\n\\n![evals diagram](https://github.com/user-attachments/assets/65c2b5af-a931-40c5-b006-87567aef019f)\\n\\n\\n\\n## Advanced RAG Techniques⚙️\\nHere are the details of all the Advanced RAG techniques covered in this repository.\\n\\n| Technique                    | Tools                        | Description                                                       | Notebooks |\\n|--------------------',\n",
       "  'filename': 'rag-cookbooks-main/README.md'},\n",
       " {'start': 3200,\n",
       "  'chunk': \"information needed to generate accurate responses.\\n\\n**Generate:** Finally, the combined query and prompt are passed to the model, which then generates the final response to the user's query.\\n\\nThese components of RAG allow the model to access up-to-date, accurate information and generate responses based on external knowledge. However, to ensure RAG systems are functioning effectively, it’s essential to evaluate their performance.\\n\\n## RAG Evaluation📊\\nEvaluating RAG applications is important for un\",\n",
       "  'filename': 'rag-cookbooks-main/README.md'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'nsive collection of advanced + agentic Retrieval-Augmented Generation (RAG) techniques.\\n\\n## Introduction🚀\\nRAG is a popular method that improves accuracy and relevance by finding the right information from reliable sources and transforming it into useful answers. This repository covers the most effective advanced + agentic RAG techniques with clear implementations and explanations.\\n\\nThe main goal of this repository is to provide a helpful resource for researchers and developers looking to use adv',\n",
       "  'filename': 'rag-cookbooks-main/README.md'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afdf549-8b0c-4c2b-9912-babffd0ea0d7",
   "metadata": {},
   "source": [
    "### Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2acb0646-cc83-4618-8238-8ce0f0112fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"What are the main four components in RAG\"\n",
    "\n",
    "\n",
    "text_results = index.search(query, num_results=3)\n",
    "\n",
    "q = embedding_model.encode(query)\n",
    "vector_results = faq_vindex.search(q, num_results=3)\n",
    "\n",
    "final_results = text_results + vector_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2d7c76d-44df-4200-9a43-7241efea5d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 2400,\n",
       " 'chunk': \"ternal documents to improve the LLM's responses through in-context learning. RAG ensures that the information provided by the LLM is not only contextually relevant but also accurate and up-to-date.\\n\\n![final diagram](https://github.com/user-attachments/assets/508b3a87-ac46-4bf7-b849-145c5465a6c0)\\n\\nThere are four main components in RAG:\\n\\n**Indexing:** First, documents (in any format) are split into chunks, and embeddings for these chunks are created. These embeddings are then added to a vector sto\",\n",
       " 'filename': 'rag-cookbooks-main/README.md'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df10bf39-3dcc-45a8-82fc-58972b6f4e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search(query):\n",
    "    return index.search(query, num_results=5)\n",
    "\n",
    "def vector_search(query):\n",
    "    q = embedding_model.encode(query)\n",
    "    return faq_vindex.search(q, num_results=5)\n",
    "\n",
    "def hybrid_search(query):\n",
    "    text_results = text_search(query)\n",
    "    vector_results = vector_search(query)\n",
    "    \n",
    "    # Combine and deduplicate results\n",
    "    seen_ids = set()\n",
    "    combined_results = []\n",
    "\n",
    "    for result in text_results + vector_results:\n",
    "        if result['filename'] not in seen_ids:\n",
    "            seen_ids.add(result['filename'])\n",
    "            combined_results.append(result)\n",
    "    \n",
    "    return combined_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9afdd975-efb6-4938-8fe1-1857d598d9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 2600,\n",
       "  'chunk': \"[final diagram](https://github.com/user-attachments/assets/508b3a87-ac46-4bf7-b849-145c5465a6c0)\\n\\nThere are four main components in RAG:\\n\\n**Indexing:** First, documents (in any format) are split into chunks, and embeddings for these chunks are created. These embeddings are then added to a vector store.\\n\\n**Retriever:** Then, the retriever finds the most relevant documents based on the user's query, using techniques like vector similarity from the vector store.\\n\\n**Augment:** After that, the Augmen\",\n",
       "  'filename': 'rag-cookbooks-main/README.md'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_search(\"What are the main components of RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689b8d62-2c8d-4f46-967f-6255a03e3b50",
   "metadata": {},
   "source": [
    "### Agents and Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ff4f644-7fad-49e5-97cb-9bfc67f62752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai \n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8be724e0-6332-4feb-93db-a6c9ce204768",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = openai.OpenAI()\n",
    "\n",
    "groq_client = Groq()\n",
    "\n",
    "user_prompt = \"What are the main components of RAG\"\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "response = groq_client.chat.completions.create(\n",
    "    messages = chat_messages,\n",
    "    model = \"openai/gpt-oss-20b\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e798c18-3308-4bd1-82b9-f0d93f9a4189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Retrieval‑Augmented Generation (RAG) – The Core Architecture\\n\\nRAG is a family of NLP systems that **blend a retrieval component with a generative language model** to answer queries, complete texts, or perform other generation tasks while grounding the output in external documents.  The design is deliberately modular so that you can swap out each part for a different implementation or tune it separately.\\n\\nBelow is a concise breakdown of the **three main building blocks** that are universally present in a RAG pipeline:\\n\\n| # | Component | Key Responsibilities | Typical Tech Choices | Why It Matters |\\n|---|-----------|----------------------|----------------------|----------------|\\n| **1** | **Retriever** | • Finds the most relevant passages / documents for a given query. <br>• Creates a set of “context snippets” that the generator will see. | • **Sparse retrievers** – BM25, TF‑IDF, ElasticSearch <br>• **Dense retrievers** – DPR, Sentence‑BERT, CLIP‑style embeddings <br>• **Hybrid** – top‑k from sparse + re‑rank with dense | Retrieval is the information‑source.  If the retrieved docs are wrong, the generation will be wrong regardless of model quality. |\\n| **2** | **Fusion / Reranker** (optional but common) | • Merges the outputs of multiple retrieval engines. <br>• Re‑scores documents (e.g., using a bi‑encoder or cross‑encoder) to surface the best context. | • Cross‑encoder rerankers (e.g., BERT‑pair, RoBERTa) <br>• Lightweight linear scorers | Improves precision of the context fed to the generator, especially in noisy retrieval scenarios. |\\n| **3** | **Generator** | • Generates the final answer or continuation conditioned on the input query **and** the retrieved context. <br>• Handles the “language modeling” side of the job. | • Transformer decoders (e.g., GPT‑2/3, LLaMA, BLOOM) <br>• Encoder‑decoder models (e.g., T5, BART, PEGASUS) <br>• RAG‑Sequence vs. RAG‑Token (different ways of incorporating context) | This is where the model’s knowledge (pre‑training) and the external data (retrieval) meet to produce a coherent output. |\\n\\n#### How They Fit Together (Typical Workflow)\\n\\n1. **Pre‑processing**  \\n   • Tokenize the user query.  \\n   • If using dense retrievers, convert the query into a vector using a fixed embedding model.\\n\\n2. **Retrieval**  \\n   • Search an index (FAISS, Elastic, Solr, etc.) and pull the top‑k passages.  \\n   • Optionally rerank these passages using a cross‑encoder.\\n\\n3. **Fusion / Context Construction**  \\n   • Concatenate the retrieved passages into a single “context string” (or feed them as separate key‑value pairs).  \\n   • Append this context to the query, or feed it into the encoder side of a seq‑2‑seq model.\\n\\n4. **Generation**  \\n   • Pass the combined input to the transformer decoder.  \\n   • The decoder can be configured in two ways:  \\n     * **RAG‑Sequence** – generates the answer *once* after seeing the entire context.  \\n     * **RAG‑Token** – generates token‑by‑token, each time re‑consulting the retriever.  \\n   • Decoding strategies (beam search, sampling, temperature control) are applied as usual.\\n\\n5. **Post‑processing**  \\n   • Filter out hallucinations (if you have a separate hallucination detector).  \\n   • Optionally re‑rank multiple generated outputs based on relevance to retrieved passages.\\n\\n---\\n\\n## Quick‑Start: A Minimal RAG Architecture in Code\\n\\n```python\\nfrom transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration\\n\\n# 1. Tokenizer and retriever (built‑in FAISS index)\\ntokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\\nretriever = RagRetriever.from_pretrained(\"facebook/rag-token-nq\")\\n\\n# 2. The generation model\\nmodel = RagSequenceForGeneration.from_pretrained(\"facebook/rag-token-nq\")\\n\\n# 3. Sample query\\nquestion = \"What are the main components of RAG?\"\\n\\n# 4. Encode the query\\ninput_ids = tokenizer(question, return_tensors=\"pt\").input_ids\\n\\n# 5. Retrieve top‑k passages\\nretrieved_docs = retriever(input_ids, return_tensors=\"pt\")\\ninput_ids = retrieved_docs[\"input_ids\"]\\n\\n# 6. Generate answer\\noutput_ids = model.generate(input_ids, num_beams=4, max_length=50)\\nanswer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\\n\\nprint(answer)\\n```\\n\\n---\\n\\n## Variations & Advanced Options\\n\\n| Variation | What it adds | Typical use‑case |\\n|-----------|--------------|------------------|\\n| **Hybrid Retrieval** | Combines BM25 + DPR top‑k → cross‑encoder rerank | Robust when the corpus contains both short FAQ‑style passages and longer documents |\\n| **Knowledge Graph Retrieval** | Instead of text snippets, pull triples from a KG | For structured QA, e.g., factoid questions |\\n| **Dynamic Context Window** | Incrementally fetch more documents when generation stalls | Useful for very large corpora or when the answer spans multiple sources |\\n| **Few‑Shot RAG** | Fine‑tune the generator with a few annotated Q‑A pairs | Adapts general RAG to a domain with limited labeled data |\\n| **RAG‑Token** | Re‑retrieves at each generation step | Helpful when the answer may require different facts as it unfolds |\\n\\n---\\n\\n## Common Pitfalls & Tips\\n\\n| Pitfall | Fix |\\n|---------|-----|\\n| **Hallucination** | Use a hallucination detector; restrict generation length; rerank generated outputs by overlap with retrieved text |\\n| **Retrieval latency** | Cache frequent queries; use approximate nearest neighbor indexes; batch queries |\\n| **Index drift** | Periodically rebuild the FAISS index to reflect corpus updates |\\n| **Too many documents** | Limit `k` to a reasonable number (often 3–10) for speed and to keep context manageable |\\n| **Context overload** | If using encoder‑decoder models, truncate or chunk the context so that it fits the model’s max sequence length |\\n\\n---\\n\\n## Summary\\n\\n- **Retriever** (dense/sparse or hybrid) pulls the relevant text from a large corpus.  \\n- **Fusion / Reranker** optionally refines that set of passages.  \\n- **Generator** (transformer decoder) produces the final answer conditioned on both query and retrieved context.\\n\\nBy understanding and tuning each of these components, you can build a RAG system that is both accurate and efficient for your specific application.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d19e784-3450-423a-9546-591733a2d067",
   "metadata": {},
   "source": [
    "## Function Calling with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3c3d3616-435b-4f9a-83ce-444b453dc7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\" : {\n",
    "    \"name\": \"text_search\",\n",
    "    \"description\": \"Search the rag cookbooks database \",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search query text to look up in the rag database\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6146c43e-d720-4618-a7e9-41a165aa3c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for answering question. \n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"What are the main components of RAG\"\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"user\", \"content\": system_prompt},\n",
    "    {'role' : 'user' , \"content\" : user_prompt}\n",
    "]\n",
    "\n",
    "response = groq_client.chat.completions.create(\n",
    "    model = \"openai/gpt-oss-20b\" ,\n",
    "    messages= chat_messages , \n",
    "    tools= [text_search_tool]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c5215f96-87ed-4bc5-b01c-caaf19584d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=None, role='assistant', executed_tools=None, function_call=None, reasoning='User asks: \"What are the main components of RAG\". We need to answer: main components of Retrieval-Augmented Generation: retrieval system, generator (LLM), re-ranking, index, prompt engineering, etc. Also mention RAG pipeline: query encoder, document retrieval, document encoder, reranker, generator. Provide explanation.', tool_calls=[ChatCompletionMessageToolCall(id='fc_aef8e937-7144-46d9-b433-899c36750f12', function=Function(arguments='{\"query\":\"main components of Retrieval-Augmented Generation RAG components\"}', name='text_search'), type='function')])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "145fa922-3972-4221-9834-473ae1c4111a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessageToolCall(id='fc_aef8e937-7144-46d9-b433-899c36750f12', function=Function(arguments='{\"query\":\"main components of Retrieval-Augmented Generation RAG components\"}', name='text_search'), type='function')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_message = response.choices[0].message\n",
    "tool_calls = response_message.tool_calls\n",
    "\n",
    "tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0deefa75-b41e-4265-909e-6aa1c3cd9978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'main components of Retrieval-Augmented Generation RAG components'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "tool_call = tool_calls[0]\n",
    "arguments = json.loads(tool_call.function.arguments)\n",
    "print(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d3768379-2585-4a6c-96e6-c87ea3720147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 1000,\n",
       "  'chunk': 'nsive collection of advanced + agentic Retrieval-Augmented Generation (RAG) techniques.\\n\\n## Introduction🚀\\nRAG is a popular method that improves accuracy and relevance by finding the right information from reliable sources and transforming it into useful answers. This repository covers the most effective advanced + agentic RAG techniques with clear implementations and explanations.\\n\\nThe main goal of this repository is to provide a helpful resource for researchers and developers looking to use adv',\n",
       "  'filename': 'rag-cookbooks-main/README.md'},\n",
       " {'start': 800,\n",
       "  'chunk': 't%20on%20GitHub:%20https://github.com/athina-ai/rag-cookbooks)\\n\\n>If you find this repository helpful, please consider giving it a star⭐️\\n\\n# Advanced + Agentic RAG Cookbooks👨🏻\\u200d💻\\nWelcome to the comprehensive collection of advanced + agentic Retrieval-Augmented Generation (RAG) techniques.\\n\\n## Introduction🚀\\nRAG is a popular method that improves accuracy and relevance by finding the right information from reliable sources and transforming it into useful answers. This repository covers the most effec',\n",
       "  'filename': 'rag-cookbooks-main/README.md'},\n",
       " {'start': 600,\n",
       "  'chunk': '?url=https://github.com/athina-ai/rag-cookbooks)&nbsp;\\n[![Share](https://img.shields.io/badge/share-FF4500?logo=reddit&logoColor=white)](https://www.reddit.com/submit?title=Check%20out%20this%20project%20on%20GitHub:%20https://github.com/athina-ai/rag-cookbooks)\\n\\n>If you find this repository helpful, please consider giving it a star⭐️\\n\\n# Advanced + Agentic RAG Cookbooks👨🏻\\u200d💻\\nWelcome to the comprehensive collection of advanced + agentic Retrieval-Augmented Generation (RAG) techniques.\\n\\n## Introduc',\n",
       "  'filename': 'rag-cookbooks-main/README.md'},\n",
       " {'start': 3200,\n",
       "  'chunk': \"information needed to generate accurate responses.\\n\\n**Generate:** Finally, the combined query and prompt are passed to the model, which then generates the final response to the user's query.\\n\\nThese components of RAG allow the model to access up-to-date, accurate information and generate responses based on external knowledge. However, to ensure RAG systems are functioning effectively, it’s essential to evaluate their performance.\\n\\n## RAG Evaluation📊\\nEvaluating RAG applications is important for un\",\n",
       "  'filename': 'rag-cookbooks-main/README.md'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'roduction to RAG💡\\nLarge Language Models are trained on a fixed dataset, which limits their ability to handle private or recent information. They can sometimes \"hallucinate\", providing incorrect yet believable answers. Fine-tuning can help but it is expensive and not ideal for retraining again and again on new data. The Retrieval-Augmented Generation (RAG) framework addresses this issue by using external documents to improve the LLM\\'s responses through in-context learning. RAG ensures that the in',\n",
       "  'filename': 'rag-cookbooks-main/README.md'}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = text_search(**arguments)\n",
    "result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "37c581c3-1dd0-44ea-9719-3dd7223158d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fc_aef8e937-7144-46d9-b433-899c36750f12'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d1783d2c-8075-45d6-b742-f0c0fc17e6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function_call_output',\n",
       " 'tool_call_id': 'fc_aef8e937-7144-46d9-b433-899c36750f12',\n",
       " 'name': 'text_search',\n",
       " 'output': '[{\"start\": 1000, \"chunk\": \"nsive collection of advanced + agentic Retrieval-Augmented Generation (RAG) techniques.\\\\n\\\\n## Introduction\\\\ud83d\\\\ude80\\\\nRAG is a popular method that improves accuracy and relevance by finding the right information from reliable sources and transforming it into useful answers. This repository covers the most effective advanced + agentic RAG techniques with clear implementations and explanations.\\\\n\\\\nThe main goal of this repository is to provide a helpful resource for researchers and developers looking to use adv\", \"filename\": \"rag-cookbooks-main/README.md\"}, {\"start\": 800, \"chunk\": \"t%20on%20GitHub:%20https://github.com/athina-ai/rag-cookbooks)\\\\n\\\\n>If you find this repository helpful, please consider giving it a star\\\\u2b50\\\\ufe0f\\\\n\\\\n# Advanced + Agentic RAG Cookbooks\\\\ud83d\\\\udc68\\\\ud83c\\\\udffb\\\\u200d\\\\ud83d\\\\udcbb\\\\nWelcome to the comprehensive collection of advanced + agentic Retrieval-Augmented Generation (RAG) techniques.\\\\n\\\\n## Introduction\\\\ud83d\\\\ude80\\\\nRAG is a popular method that improves accuracy and relevance by finding the right information from reliable sources and transforming it into useful answers. This repository covers the most effec\", \"filename\": \"rag-cookbooks-main/README.md\"}, {\"start\": 600, \"chunk\": \"?url=https://github.com/athina-ai/rag-cookbooks)&nbsp;\\\\n[![Share](https://img.shields.io/badge/share-FF4500?logo=reddit&logoColor=white)](https://www.reddit.com/submit?title=Check%20out%20this%20project%20on%20GitHub:%20https://github.com/athina-ai/rag-cookbooks)\\\\n\\\\n>If you find this repository helpful, please consider giving it a star\\\\u2b50\\\\ufe0f\\\\n\\\\n# Advanced + Agentic RAG Cookbooks\\\\ud83d\\\\udc68\\\\ud83c\\\\udffb\\\\u200d\\\\ud83d\\\\udcbb\\\\nWelcome to the comprehensive collection of advanced + agentic Retrieval-Augmented Generation (RAG) techniques.\\\\n\\\\n## Introduc\", \"filename\": \"rag-cookbooks-main/README.md\"}, {\"start\": 3200, \"chunk\": \"information needed to generate accurate responses.\\\\n\\\\n**Generate:** Finally, the combined query and prompt are passed to the model, which then generates the final response to the user\\'s query.\\\\n\\\\nThese components of RAG allow the model to access up-to-date, accurate information and generate responses based on external knowledge. However, to ensure RAG systems are functioning effectively, it\\\\u2019s essential to evaluate their performance.\\\\n\\\\n## RAG Evaluation\\\\ud83d\\\\udcca\\\\nEvaluating RAG applications is important for un\", \"filename\": \"rag-cookbooks-main/README.md\"}, {\"start\": 2000, \"chunk\": \"roduction to RAG\\\\ud83d\\\\udca1\\\\nLarge Language Models are trained on a fixed dataset, which limits their ability to handle private or recent information. They can sometimes \\\\\"hallucinate\\\\\", providing incorrect yet believable answers. Fine-tuning can help but it is expensive and not ideal for retraining again and again on new data. The Retrieval-Augmented Generation (RAG) framework addresses this issue by using external documents to improve the LLM\\'s responses through in-context learning. RAG ensures that the in\", \"filename\": \"rag-cookbooks-main/README.md\"}]'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_output = {\n",
    "    \"type\": \"function_call_output\",\n",
    "    \"tool_call_id\" : tool_call.id,\n",
    "    \"name\": tool_call.function.name,\n",
    "    \"output\": json.dumps(result),\n",
    "}\n",
    "\n",
    "call_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "554eec85-abd9-4435-a184-9c2d6ac09bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "80865db3-c9fa-4e7c-bc50-2a8f63761884",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course. \n",
    "\n",
    "Always search for relevant information before answering. \n",
    "If the first search doesn't give you enough information, try different search terms.\n",
    "\n",
    "Make multiple searches if needed to provide comprehensive answers.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5be6eec9-dd1c-456a-9a52-2600d6367091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "\n",
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the RAG cookbook data.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of up to 5 search results returned by the RAG cookbook.\n",
    "    \"\"\"\n",
    "    return index.search(query, num_results=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "913d48bc-c581-4955-87ab-60ffeb02b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from pydantic_ai import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8526bb65-0dd8-48c1-9a20-62a0a073b6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    name=\"rag_agent\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='gpt-4o-mini'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "73e89502-e6a0-430c-a39b-cbae372e5fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the main components of RAG\"\n",
    "\n",
    "result = await agent.run(user_prompt=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b9f6ed02-90ef-4a05-ad0b-fbbf766f3dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentRunResult(output=\"Retrieval-Augmented Generation (RAG) consists of four main components:\\n\\n1. **Indexing**: Initially, documents are divided into smaller chunks, and embeddings for these chunks are generated. These embeddings are stored in a vector store, which allows for efficient retrieval based on a query.\\n\\n2. **Retrieval**: When a query is made, the system retrieves relevant document chunks from the vector store. This step is crucial for ensuring that the response generated is based on the most pertinent and up-to-date information.\\n\\n3. **Generation**: The retrieved document chunks are then used as context for a language model (like GPT) to generate responses. The model combines the information from these document chunks with its own learned knowledge to provide a coherent and relevant answer.\\n\\n4. **In-context Learning**: This component enhances the model's ability to use external documents effectively in generating responses, ensuring that the outputs are contextually relevant and accurate.\\n\\nThese components work together to improve the quality of answers provided by language models by incorporating real-time and specific information from reliable sources.\")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13078aed-a8c3-43dd-9882-3ff2c9e029d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
